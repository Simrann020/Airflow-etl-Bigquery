# Airflow ETL with BigQuery

This project implements an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** and **Google BigQuery**.  
It is designed to automate data ingestion, transformation, and loading processes for scalable analytics.

---

## Features
- **Apache Airflow** DAGs to orchestrate ETL workflows  
- Integration with **Google BigQuery** for storage and querying  
- **Dockerized environment** with `docker-compose` for easy local setup  
- Configurable via `.env` and externalized secrets (not committed to Git)  
- Modular structure for adding new DAGs and plugins  

---

## ğŸ“‚ Project Structure
```
airflow-etl-bigquery/
â”‚â”€â”€ dags/                  # Airflow DAG definitions
â”‚â”€â”€ data/                  # Raw/processed data (local only, not in Git)
â”‚â”€â”€ include/               # Helper scripts and configs
â”‚   â””â”€â”€ gcp-key.json       # GCP service account key
â”‚â”€â”€ logs/                  # Airflow logs
â”‚â”€â”€ plugins/               # Custom Airflow operators & hooks
â”‚â”€â”€ .env                   # Environment variables
â”‚â”€â”€ docker-compose.yml     # Dockerized Airflow setup
â”‚â”€â”€ Dockerfile             # Custom Airflow image
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ README.md              # Project documentation
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/Simrann020/Airflow-etl-Bigquery.git
cd airflow-etl-bigquery
```

### 2. Setup Environment Variables
- Copy `.env.example` to `.env`
- Fill in required environment variables (BigQuery project ID, dataset, etc.)

### 3. Start Airflow
```bash
docker-compose up --build
```

Airflow UI will be available at [http://localhost:8080](http://localhost:8080).

### 4. Run a DAG
- Trigger DAGs from Airflow UI  
- Monitor logs and execution status
